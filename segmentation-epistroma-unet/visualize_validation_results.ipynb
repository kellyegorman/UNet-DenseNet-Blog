{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kellyegorman/UNet-DenseNet-Blog/blob/main/segmentation-epistroma-unet/visualize_validation_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQa5pl1SrmFG",
        "outputId": "bfd70905-b9f9-49ce-9dcb-28eae2bb3393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/PytorchDigitalPathology/segmentation_epistroma_unet\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/PytorchDigitalPathology/segmentation_epistroma_unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nP9CcDdjnPn"
      },
      "outputs": [],
      "source": [
        "#v1\n",
        "#26/10/2018\n",
        "\n",
        "dataname=\"epistroma\" #should match the value used to train the network, will be used to load the appropirate model\n",
        "gpuid=0\n",
        "\n",
        "\n",
        "patch_size=256 #should match the value used to train the network\n",
        "batch_size=1 #nicer to have a single batch so that we can iterately view the output, while not consuming too much\n",
        "edge_weight=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC5JqRbQj65j",
        "outputId": "2dc01ac9-2a98-46b7-b5a3-121ebd05186e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmIjV66Oj3G9"
      },
      "outputs": [],
      "source": [
        "import random, sys\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.ndimage\n",
        "import skimage\n",
        "import time\n",
        "\n",
        "import tables\n",
        "from skimage import io, morphology\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from unet import UNet #code borrowed from https://github.com/jvanvugt/pytorch-unet\n",
        "import PIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crfRm9fuklPz",
        "outputId": "2bfe52ab-37f8-4b36-f39a-53565581ee01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15095MB, multi_processor_count=40, uuid=9ceb3069-6415-ed3e-1a21-da54a14ad30d, L2_cache_size=4MB)\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.get_device_properties(gpuid))\n",
        "torch.cuda.set_device(gpuid)\n",
        "device = torch.device(f'cuda:{gpuid}' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgfNmVsulG28",
        "outputId": "0224806b-edf1-44cf-ce6f-321f27583145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fb403b1de4a6>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(f\"{dataname}_unet_best_model.pth\")\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load(f\"{dataname}_unet_best_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7dWNBmwlNTH",
        "outputId": "9060df92-7383-4721-a7ea-a8adb94dbc1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total params: \t122466\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#load the model, note that the paramters are coming from the checkpoint, since the architecture of the model needs to exactly match the weights saved\n",
        "model = UNet(n_classes=checkpoint[\"n_classes\"], in_channels=checkpoint[\"in_channels\"], padding=checkpoint[\"padding\"],depth=checkpoint[\"depth\"],\n",
        "             wf=checkpoint[\"wf\"], up_mode=checkpoint[\"up_mode\"], batch_norm=checkpoint[\"batch_norm\"]).to(device)\n",
        "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n",
        "model.load_state_dict(checkpoint[\"model_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1gRfDaClS4H"
      },
      "outputs": [],
      "source": [
        "#this defines our dataset class which will be used by the dataloader\n",
        "class Dataset(object):\n",
        "    def __init__(self, fname ,img_transform=None, mask_transform = None, edge_weight= False):\n",
        "        #nothing special here, just internalizing the constructor parameters\n",
        "        self.fname=fname\n",
        "        self.edge_weight = edge_weight\n",
        "\n",
        "        self.img_transform=img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        self.tables=tables.open_file(self.fname)\n",
        "        self.numpixels=self.tables.root.numpixels[:]\n",
        "        self.nitems=self.tables.root.img.shape[0]\n",
        "        self.tables.close()\n",
        "\n",
        "        self.img = None\n",
        "        self.mask = None\n",
        "    def __getitem__(self, index):\n",
        "        #opening should be done in __init__ but seems to be\n",
        "        #an issue with multithreading so doing here\n",
        "        if(self.img is None): #open in thread\n",
        "            self.tables=tables.open_file(self.fname)\n",
        "            self.img=self.tables.root.img\n",
        "            self.mask=self.tables.root.mask\n",
        "\n",
        "        #get the requested image and mask from the pytable\n",
        "        img = self.img[index,:,:,:]\n",
        "        mask = self.mask[index,:,:]\n",
        "\n",
        "        #the original Unet paper assignes increased weights to the edges of the annotated objects\n",
        "        #their method is more sophistocated, but this one is faster, we simply dilate the mask and\n",
        "        #highlight all the pixels which were \"added\"\n",
        "        if(self.edge_weight):\n",
        "            weight = scipy.ndimage.morphology.binary_dilation(mask==1, iterations =2) & ~mask\n",
        "        else: #otherwise the edge weight is all ones and thus has no affect\n",
        "            weight = np.ones(mask.shape,dtype=mask.dtype)\n",
        "\n",
        "        mask = mask[:,:,None].repeat(3,axis=2) #in order to use the transformations given by torchvision\n",
        "        weight = weight[:,:,None].repeat(3,axis=2) #inputs need to be 3D, so here we convert from 1d to 3d by repetition\n",
        "\n",
        "        img_new = img\n",
        "        mask_new = mask\n",
        "        weight_new = weight\n",
        "\n",
        "        seed = random.randrange(sys.maxsize) #get a random seed so that we can reproducibly do the transofrmations\n",
        "        if self.img_transform is not None:\n",
        "            random.seed(seed) # apply this seed to img transforms\n",
        "            img_new = self.img_transform(img)\n",
        "\n",
        "        if self.mask_transform is not None:\n",
        "            random.seed(seed)\n",
        "            mask_new = self.mask_transform(mask)\n",
        "            mask_new = np.asarray(mask_new)[:,:,0].squeeze()\n",
        "\n",
        "            random.seed(seed)\n",
        "            weight_new = self.mask_transform(weight)\n",
        "            weight_new = np.asarray(weight_new)[:,:,0].squeeze()\n",
        "\n",
        "        return img_new, mask_new, weight_new\n",
        "    def __len__(self):\n",
        "        return self.nitems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7jsbkyelUm1"
      },
      "outputs": [],
      "source": [
        "#note that since we need the transofrmations to be reproducible for both masks and images\n",
        "#we do the spatial transformations first, and afterwards do any color augmentations\n",
        "\n",
        "#in the case of using this for output generation, we want to use the original images since they will give a better sense of the exepected\n",
        "#output when used on the rest of the dataset, as a result, we disable all unnecessary augmentation.\n",
        "#the only component that remains here is the randomcrop, to ensure that regardless of the size of the image\n",
        "#in the database, we extract an appropriately sized patch\n",
        "img_transform = transforms.Compose([\n",
        "     transforms.ToPILImage(),\n",
        "    #transforms.RandomVerticalFlip(),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\n",
        "    #transforms.RandomResizedCrop(size=patch_size),\n",
        "    #transforms.RandomRotation(180),\n",
        "    #transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=.5),\n",
        "    #transforms.RandomGrayscale(),\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    #transforms.RandomVerticalFlip(),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\n",
        "    #transforms.RandomResizedCrop(size=patch_size,interpolation=PIL.Image.NEAREST),\n",
        "    #transforms.RandomRotation(180),\n",
        "    ])\n",
        "\n",
        "phases=[\"val\"]\n",
        "dataset={}\n",
        "dataLoader={}\n",
        "for phase in phases:\n",
        "\n",
        "    dataset[phase]=Dataset(f\"./{dataname}_{phase}.pytable\", img_transform=img_transform , mask_transform = mask_transform ,edge_weight=edge_weight)\n",
        "    dataLoader[phase]=DataLoader(dataset[phase], batch_size=batch_size,\n",
        "                                shuffle=True, num_workers=0, pin_memory=True) #,pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQf1jjWfnoII"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "#set the model to evaluation mode, since we're only generating output and not doing any back propogation\n",
        "model.eval()\n",
        "for ii , (X, y, y_weight) in enumerate(dataLoader[\"val\"]):\n",
        "    X = X.to(device)  # [NBATCH, 3, H, W]\n",
        "    y = y.type('torch.LongTensor').to(device)  # [NBATCH, H, W] with class indices (0, 1)\n",
        "\n",
        "    output = model(X)  # [NBATCH, 2, H, W]\n",
        "\n",
        "    output=output.detach().squeeze().cpu().numpy() #get output and pull it to CPU\n",
        "    output=np.moveaxis(output,0,-1)  #reshape moving last dimension\n",
        "\n",
        "    fig, ax = plt.subplots(1,4, figsize=(10,4))  # 1 row, 2 columns\n",
        "\n",
        "    ax[0].imshow(output[:,:,1])\n",
        "    ax[1].imshow(np.argmax(output,axis=2))\n",
        "    ax[2].imshow(y.detach().squeeze().cpu().numpy())\n",
        "    ax[3].imshow(np.moveaxis(X.detach().squeeze().cpu().numpy(),0,-1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIiNOLiSXcCk"
      },
      "source": [
        "**CREATE UNET PPTX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrIN2WSGKrvg",
        "outputId": "f3feb8e9-d02e-4757-d2b8-99255dba9f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.12.2)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/472.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/165.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.2 python-pptx-1.0.2\n"
          ]
        }
      ],
      "source": [
        "pip install python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAKvWUCZVqjt",
        "outputId": "db7a8b71-12b3-44cb-997d-7a948217e259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-d16cf567aa77>:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Inches\n",
        "\n",
        "from datetime import datetime\n",
        "from skimage import color\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from tqdm.autonotebook import tqdm\n",
        "import PIL.Image as Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-pY8vY-V1yE"
      },
      "outputs": [],
      "source": [
        "# -- Set meta data which will appear on first slide\n",
        "title = \"Epi/stroma segmentation\"\n",
        "date = datetime.today()\n",
        "author = \"Kelly Gorman\"\n",
        "comments = \"data and code taken from blog andrewjanowczyk.com \"\n",
        "pptxfname = \"epistroma_results.pptx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_WNk7X_V5Id"
      },
      "outputs": [],
      "source": [
        "mask_files=glob.glob('./data/masks/*.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAXeK7EqWQ8g"
      },
      "outputs": [],
      "source": [
        "#create presentation\n",
        "prs = Presentation()\n",
        "prs.slide_width = Inches(10)\n",
        "prs.slide_height = Inches(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwZxqj36WTF4"
      },
      "outputs": [],
      "source": [
        "blank_slide_layout = prs.slide_layouts[1]\n",
        "slide = prs.slides.add_slide(blank_slide_layout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAptDZqEWVU_"
      },
      "outputs": [],
      "source": [
        "#make first slide with our metadata\n",
        "slide.placeholders[0].text = title\n",
        "\n",
        "tf = slide.placeholders[1].text_frame\n",
        "tf.text = f'Date: {date}\\n'\n",
        "tf.text += f\"Author: {author}\\n\"\n",
        "tf.text += f\"Comments: {comments}\\n\"\n",
        "\n",
        "# -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ork3sjcQWZsy"
      },
      "outputs": [],
      "source": [
        "#wrapper function to add an image as a byte stream to a slide\n",
        "#note that this is in place of having to save output directly to disk, and can be used in dynamic settings as well\n",
        "def addimagetoslide(slide,img,left,top, height, width, resize = .5):\n",
        "    res = cv2.resize(img , None, fx=resize,fy=resize ,interpolation=cv2.INTER_CUBIC) #since the images are going to be small, we can resize them to prevent the final pptx file from being large for no reason\n",
        "    image_stream = BytesIO()\n",
        "    Image.fromarray(res).save(image_stream,format=\"PNG\")\n",
        "\n",
        "    pic = slide.shapes.add_picture(image_stream, left, top ,height,width)\n",
        "    image_stream.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi56uJ2kWeEd"
      },
      "outputs": [],
      "source": [
        "#helper function to blend two images\n",
        "def blend2Images(img, mask):\n",
        "    if (img.ndim == 3):\n",
        "        img = color.rgb2gray(img)\n",
        "    if (mask.ndim == 3):\n",
        "        mask = color.rgb2gray(mask)\n",
        "    img = img[:, :, None] * 1.0  # can't use boolean\n",
        "    mask = mask[:, :, None] * 1.0\n",
        "    out = np.concatenate((mask, img, mask), 2) * 255\n",
        "    return out.astype('uint8')\n",
        "\n",
        "\n",
        "# +\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "2285e08d7c7247f79c9baeec1840aae1",
            "cac92615ff1842e6b793e0d7ec38ceaa",
            "0808a693b76d4c01a65b4e600bce335a",
            "3b1b5745861944b491a2aa74ad80d607",
            "fa267784a8b240b084ce9b2b55a04b82",
            "9bfa1c84635e495fbea94bd929e71a68",
            "f7418464383e4d07aa223b20fe882e44",
            "ea3df59638f44e75a756dbf4b9266014",
            "3966983099db41a1adb8763f704a10d3",
            "3efef37055544cfa823df81f5409f3f8",
            "8171b1c845014aa0b28c810738e7218d"
          ]
        },
        "id": "Kvvj08XpWf8V",
        "outputId": "0fc41fe0-8b82-4a2c-e9f3-f1379106ed7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/42 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2285e08d7c7247f79c9baeec1840aae1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# +\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches\n",
        "\n",
        "from datetime import datetime\n",
        "from skimage import color\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from tqdm.autonotebook import tqdm\n",
        "import PIL.Image as Image\n",
        "import os\n",
        "\n",
        "# Ensure correct base path\n",
        "base_path = \"/gdrive/MyDrive/PytorchDigitalPathology/segmentation_epistroma_unet/data\"\n",
        "\n",
        "# Check if running in Google Colab and mount Drive\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = \"/content/drive/MyDrive/PytorchDigitalPathology/segmentation_epistroma_unet/data\"\n",
        "\n",
        "# +\n",
        "# -- Set meta data which will appear on first slide\n",
        "title = \"Epi/stroma segmentation\"\n",
        "date = datetime.today()\n",
        "author = \"Kelly Gorman\"\n",
        "comments = \"data and code taken from blog andrewjanowczyk.com \"\n",
        "pptxfname = \"/content/drive/MyDrive/epistroma_results.pptx\"\n",
        "\n",
        "# Only process images that have masks\n",
        "mask_files = glob.glob(f\"{base_path}/masks/*.png\")\n",
        "\n",
        "# +\n",
        "# Create presentation\n",
        "prs = Presentation()\n",
        "prs.slide_width = Inches(10)\n",
        "prs.slide_height = Inches(10)\n",
        "\n",
        "blank_slide_layout = prs.slide_layouts[1]\n",
        "slide = prs.slides.add_slide(blank_slide_layout)\n",
        "\n",
        "# Add metadata to first slide\n",
        "slide.placeholders[0].text = title\n",
        "\n",
        "tf = slide.placeholders[1].text_frame\n",
        "tf.text = f'Date: {date}\\n'\n",
        "tf.text += f\"Author: {author}\\n\"\n",
        "tf.text += f\"Comments: {comments}\\n\"\n",
        "\n",
        "# -\n",
        "\n",
        "# Wrapper function to add an image as a byte stream to a slide\n",
        "def addimagetoslide(slide, img, left, top, height, width, resize=0.5):\n",
        "    res = cv2.resize(img, None, fx=resize, fy=resize, interpolation=cv2.INTER_CUBIC)  # Resize images to reduce pptx size\n",
        "    image_stream = BytesIO()\n",
        "    Image.fromarray(res).save(image_stream, format=\"PNG\")\n",
        "\n",
        "    pic = slide.shapes.add_picture(image_stream, left, top, height, width)\n",
        "    image_stream.close()\n",
        "\n",
        "\n",
        "# Helper function to blend two images\n",
        "def blend2Images(img, mask):\n",
        "    if img.ndim == 3:\n",
        "        img = color.rgb2gray(img)\n",
        "    if mask.ndim == 3:\n",
        "        mask = color.rgb2gray(mask)\n",
        "    img = img[:, :, None] * 1.0  # Can't use boolean\n",
        "    mask = mask[:, :, None] * 1.0\n",
        "    out = np.concatenate((mask, img, mask), 2) * 255\n",
        "    return out.astype('uint8')\n",
        "\n",
        "\n",
        "# +\n",
        "for mask_fname in tqdm(mask_files):\n",
        "    # Add a new slide for this set of images\n",
        "    blank_slide_layout = prs.slide_layouts[0]\n",
        "    slide = prs.slides.add_slide(blank_slide_layout)\n",
        "\n",
        "    # Compute associated filenames\n",
        "    orig_fname = mask_fname.replace(f\"{base_path}/masks\", base_path).replace(\"_mask.png\", \".tif\")\n",
        "    output_fname = mask_fname.replace(f\"{base_path}/masks\", f\"{base_path}/output\").replace(\"_mask.png\", \"_class.png\")\n",
        "\n",
        "    # Check if original file exists before proceeding\n",
        "    if not os.path.exists(orig_fname):\n",
        "        print(f\"Warning: {orig_fname} not found. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # ------- Load and add original image\n",
        "    img = cv2.cvtColor(cv2.imread(orig_fname), cv2.COLOR_BGR2RGB)\n",
        "    addimagetoslide(slide, img, Inches(0), Inches(0), Inches(5), Inches(5))\n",
        "\n",
        "    # ------ Load and add mask\n",
        "    mask = cv2.cvtColor(cv2.imread(mask_fname), cv2.COLOR_BGR2RGB)\n",
        "    addimagetoslide(slide, mask, Inches(5), Inches(0), Inches(5), Inches(5))\n",
        "\n",
        "    # ------ Load and add output\n",
        "    if os.path.exists(output_fname):  # Ensure output file exists\n",
        "        output = cv2.cvtColor(cv2.imread(output_fname), cv2.COLOR_BGR2RGB)\n",
        "        addimagetoslide(slide, output, Inches(5), Inches(5), Inches(5), Inches(5))\n",
        "    else:\n",
        "        print(f\"Warning: {output_fname} not found. Skipping output image.\")\n",
        "\n",
        "    # ------ Fuse - load and add to slide\n",
        "    addimagetoslide(slide, blend2Images(output, mask), Inches(0), Inches(5), Inches(5), Inches(5))\n",
        "\n",
        "    # ------ Add results text box\n",
        "    txBox = slide.shapes.add_textbox(Inches(10), Inches(0), Inches(4), Inches(4))\n",
        "    tf = txBox.text_frame\n",
        "    tf.text = f\"{orig_fname}\\n\"\n",
        "    tf.text += f\"Overall Pixel Agreement: {(output == mask).mean():.4f}\\n\"\n",
        "    tf.text += f\"True Positive Rate: {(mask[output > 0] > 0).sum() / (output > 0).sum():.4f}\\n\"\n",
        "    tf.text += f\"False Positive Rate: {(mask[output == 0] > 0).sum() / (output == 0).sum():.4f}\\n\"\n",
        "    tf.text += f\"True Negative Rate: {(mask[output == 0] == 0).sum() / (output == 0).sum():.4f}\\n\"\n",
        "    tf.text += f\"False Negative Rate: {(mask[output > 0] == 0).sum() / (output > 0).sum():.4f}\\n\"\n",
        "\n",
        "# Save presentation\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "prs.save(pptxfname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ9eqtxKWoI1",
        "outputId": "5024702a-09d2-4318-d3b3-e7bf3a7f8cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: ['10258_00006.tif', '10277_00006.tif', '10262_00025.tif', '10269_00022.tif', '10276_00037.tif', '10260_00022.tif', '10261_00002.tif', '10256_00003.tif', '10275_00099.tif', '10264_00056.tif', '10274_00004.tif', '10273_00095.tif', '10259_00002.tif', '10257_00026.tif', '10254_00001.tif', '10282_00016.tif', '10301_00026.tif', '10292_00018.tif', '10288_00003.tif', '10279_00049.tif', '10302_00098.tif', '10293_00011.tif', '10291_00012.tif', '10285_00007.tif', '10278_00006.tif', '10299_00154.tif', '10286_00014.tif', '10295_00012.tif', '10304_00005.tif', '12752_00004.tif', '10303_00090.tif', '10307_00004.tif', '12819_00004.tif', '12820_00005.tif', '12811_00008.tif', '12818_00006.tif', '12626_00016.tif', '10308_00048.tif', '12749_00010.tif', '10306_00018.tif', '12900_00008.tif', '12906_00017.tif', '12821_00018.tif', '12882_00026.tif', '12881_00009.tif', '12875_00002.tif', '12822_00002.tif', '12867_00005.tif', '12880_00001.tif', '12901_00005.tif', '12826_00003.tif', '12884_00018.tif', '12891_00006.tif', '8914_00006.tif', '8915_00001.tif', '12929_00017.tif', '8951_00005.tif', '8916_00005.tif', '8913_00002.tif', '12934_00025.tif', '8864_00024.tif', '12948_00001.tif', '12932_00003.tif', '8918_00007.tif', '12909_00003.tif', '12930_00008.tif', '8867_00011.tif', '12947_00004.tif', '12911_00005.tif', '12931_00010.tif', '12907_00003.tif', '8865_00002.tif', '8956_00011.tif', '9043_00045.tif', '9022_00008.tif', '8957_00005.tif', '9081_00008.tif', '9041_00008.tif', '9037_00009.tif', '8974_00014.tif', '9083_00022.tif', '8975_00017.tif', '9024_00001.tif', '8959_00013.tif', '9076_00005.tif', '8980_00013.tif', '9123_00005.tif', '9023_00012.tif', '8955_00018.tif', '9029_00010.tif', '9078_00011.tif', '9073_00007.tif', '9178_00010.tif', '9261_00019.tif', '9255_00011.tif', '9254_00013.tif', '9228_00003.tif', '9173_00049.tif', '9259_00017.tif', '9227_00006.tif', '9257_00012.tif', '9176_00024.tif', '9250_00025.tif', '9181_00013.tif', '9175_00001.tif', '9225_00002.tif', '9125_00005.tif', '9124_00021.tif', '9177_00004.tif', '9174_00001.tif', '9256_00054.tif', '9126_00056.tif', '9323_00021.tif', '9382_00020.tif', '9346_00019.tif', '9383_00006.tif', '9322_00006.tif', '9345_00065.tif', '9267_00024.tif', '9290_00006.tif', '9265_00001.tif', '9266_00002.tif', '9381_00010.tif', '9347_00011.tif', '9320_00011.tif', 'masks', 'output']\n",
            "Masks: ['10264_00056_mask.png', '10261_00002_mask.png', '10288_00003_mask.png', '10293_00011_mask.png', '10285_00007_mask.png', '10278_00006_mask.png', '10286_00014_mask.png', '10260_00022_mask.png', '10279_00049_mask.png', '10282_00016_mask.png', '10291_00012_mask.png', '12826_00003_mask.png', '12819_00004_mask.png', '10302_00098_mask.png', '12867_00005_mask.png', '10304_00005_mask.png', '10295_00012_mask.png', '12875_00002_mask.png', '12818_00006_mask.png', '12820_00005_mask.png', '12626_00016_mask.png', '12929_00017_mask.png', '12911_00005_mask.png', '10308_00048_mask.png', '12907_00003_mask.png', '12909_00003_mask.png', '12880_00001_mask.png', '12884_00018_mask.png', '12930_00008_mask.png', '12881_00009_mask.png', '8918_00007_mask.png', '8975_00017_mask.png', '9023_00012_mask.png', '12947_00004_mask.png', '9227_00006_mask.png', '9250_00025_mask.png', '8974_00014_mask.png', '8957_00005_mask.png', '9043_00045_mask.png', '8951_00005_mask.png', '12932_00003_mask.png', '9346_00019_mask.png']\n",
            "Outputs: ['10258_00006_class.png', '10277_00006_class.png', '10262_00025_class.png', '10269_00022_class.png', '10276_00037_class.png', '10260_00022_class.png', '10261_00002_class.png', '10256_00003_class.png', '10275_00099_class.png', '10264_00056_class.png', '10274_00004_class.png', '10273_00095_class.png', '10259_00002_class.png', '10257_00026_class.png', '10254_00001_class.png', '10282_00016_class.png', '10301_00026_class.png', '10292_00018_class.png', '10288_00003_class.png', '10279_00049_class.png', '10302_00098_class.png', '10293_00011_class.png', '10291_00012_class.png', '10285_00007_class.png', '10278_00006_class.png', '10299_00154_class.png', '10286_00014_class.png', '10295_00012_class.png', '10304_00005_class.png', '12752_00004_class.png', '10303_00090_class.png', '10307_00004_class.png', '12819_00004_class.png', '12820_00005_class.png', '12811_00008_class.png', '12818_00006_class.png', '12626_00016_class.png', '10308_00048_class.png', '12749_00010_class.png', '10306_00018_class.png', '12900_00008_class.png', '12906_00017_class.png', '12821_00018_class.png', '12882_00026_class.png', '12881_00009_class.png', '12875_00002_class.png', '12822_00002_class.png', '12867_00005_class.png', '12880_00001_class.png', '12901_00005_class.png', '12826_00003_class.png', '12884_00018_class.png', '12891_00006_class.png', '8914_00006_class.png', '8915_00001_class.png', '12929_00017_class.png', '8951_00005_class.png', '8916_00005_class.png', '8913_00002_class.png', '12934_00025_class.png', '8864_00024_class.png', '12948_00001_class.png', '12932_00003_class.png', '8918_00007_class.png', '12909_00003_class.png', '12930_00008_class.png', '8867_00011_class.png', '12947_00004_class.png', '12911_00005_class.png', '12931_00010_class.png', '12907_00003_class.png', '8865_00002_class.png', '8956_00011_class.png', '9043_00045_class.png', '9022_00008_class.png', '8957_00005_class.png', '9081_00008_class.png', '9041_00008_class.png', '9037_00009_class.png', '8974_00014_class.png', '9083_00022_class.png', '8975_00017_class.png', '9024_00001_class.png', '8959_00013_class.png', '9076_00005_class.png', '8980_00013_class.png', '9123_00005_class.png', '9023_00012_class.png', '8955_00018_class.png', '9029_00010_class.png', '9078_00011_class.png', '9073_00007_class.png', '9178_00010_class.png', '9261_00019_class.png', '9255_00011_class.png', '9254_00013_class.png', '9228_00003_class.png', '9173_00049_class.png', '9259_00017_class.png', '9227_00006_class.png', '9257_00012_class.png', '9176_00024_class.png', '9250_00025_class.png', '9181_00013_class.png', '9175_00001_class.png', '9225_00002_class.png', '9125_00005_class.png', '9124_00021_class.png', '9177_00004_class.png', '9174_00001_class.png', '9256_00054_class.png', '9126_00056_class.png', '9323_00021_class.png', '9382_00020_class.png', '9346_00019_class.png', '9383_00006_class.png', '9322_00006_class.png', '9345_00065_class.png', '9267_00024_class.png', '9290_00006_class.png', '9265_00001_class.png', '9266_00002_class.png', '9381_00010_class.png', '9347_00011_class.png', '9320_00011_class.png']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Images:\", os.listdir('./data'))\n",
        "print(\"Masks:\", os.listdir('./data/masks'))\n",
        "print(\"Outputs:\", os.listdir('./data/output'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4tKYIfh5geF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2285e08d7c7247f79c9baeec1840aae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cac92615ff1842e6b793e0d7ec38ceaa",
              "IPY_MODEL_0808a693b76d4c01a65b4e600bce335a",
              "IPY_MODEL_3b1b5745861944b491a2aa74ad80d607"
            ],
            "layout": "IPY_MODEL_fa267784a8b240b084ce9b2b55a04b82"
          }
        },
        "cac92615ff1842e6b793e0d7ec38ceaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bfa1c84635e495fbea94bd929e71a68",
            "placeholder": "​",
            "style": "IPY_MODEL_f7418464383e4d07aa223b20fe882e44",
            "value": "100%"
          }
        },
        "0808a693b76d4c01a65b4e600bce335a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea3df59638f44e75a756dbf4b9266014",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3966983099db41a1adb8763f704a10d3",
            "value": 42
          }
        },
        "3b1b5745861944b491a2aa74ad80d607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3efef37055544cfa823df81f5409f3f8",
            "placeholder": "​",
            "style": "IPY_MODEL_8171b1c845014aa0b28c810738e7218d",
            "value": " 42/42 [00:10&lt;00:00,  4.66it/s]"
          }
        },
        "fa267784a8b240b084ce9b2b55a04b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bfa1c84635e495fbea94bd929e71a68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7418464383e4d07aa223b20fe882e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea3df59638f44e75a756dbf4b9266014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3966983099db41a1adb8763f704a10d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3efef37055544cfa823df81f5409f3f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8171b1c845014aa0b28c810738e7218d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}